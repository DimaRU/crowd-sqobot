; Initial values for any options omitted in more specific config files.
;
; Please do not edit this file to avoid problems when upgrading.
; Use user-level files like main.conf instead.
;

;------------------------------------------------------------------------
; Database
;------------------------------------------------------------------------

dbDSN             = mysql:host=localhost;dbname=crowd-crawler
dbUser            = crowd
dbPassword        = ergomost
dbPrefix          = st_
dbEngine          = InnoDB

dbConCharset      = utf8

; Affects DATETIME fields. Empty value equals to 'timeZone' setting or default
; PHP value if 'timeZone' is empty.
; This generally is a TZ identifier ("UTC") or a string like +0:00.
;
dbConTimeZone     = 

; All but '' and 0 values enable off-loaded table listing all per-site IDs that
; should not be crawled. Useful for distributed spider to keep all instances in
; sync. If '1' table name is set to 'pages'. Final table value starts with dbPrefix.
;
dbPageIndex       =

; Enables logging of executed queries. strftime() %X substrings can be used
; similar to logFile.
dbLog             = 
;out/log.sql

; When dbLog exceeds this size it's cleared. Allows for K/M suffixes (Kilo/Megabyte).
dbLogMax          = 10M


;------------------------------------------------------------------------
; Miscellaneous
;------------------------------------------------------------------------

; For date_default_timezone_set(). Highly recommended to match this and dbConTimeZone.
; See http://www.php.net/manual/en/timezones.php for possible values.
; Empty value uses default PHP timezone (from php.ini or elsewhere).
;
timeZone          = UTC

; Seconds after which if a queue item hasn't been fulfilled it's marked as failed.
; Usually Sqobot will mark failed items even on Fatal Errors but if things really
; went wild it might have no chance - in this case items that have timed out are
; marked as such by another process in the beginning of their queue processing job.
;
queueTimeout      = 60

log               = info warn error fatal
logFile           = out/%Y-%m-%d-%a.log

mailFrom          = root@localhost


;------------------------------------------------------------------------
; Remote Server Connections
;------------------------------------------------------------------------

; Msec to wait before doing repetitive remote server requests. A random 10% margin
; is added so that for 200 msec actual delay will be between 200 and 220 msec.
remoteDelay       = 50

; Lists of values to be randomly picked when contacting remote server.
;
; Accept-Language header
dl languages      = en-us en-gb en de ja cn-zh pt-br es it ru
; Accept-Charset header
dl charsets       = utf-8 iso-8859-1 windows-1251 koi8-u big-5 euc-jp shift-jis
; Accept header
dl mimes          = text/html application/xhtml+xml application/xml
; User-Agent
dl useragent      = Mozilla/5.0 (Windows NT 6.1; WOW64; rv:24.0) Gecko/20100101 Firefox/24.0

dlRedirects       = 5
; HTTP/1.0 or 1.1.
dlProtocol        = 1.0
; Seconds. Can be float, e.g. 3.5 for 3500 msec. It also affects the timeout of
; inter-node requests.
dlTimeout         = 30
; If set HTTP error statuses (404, 500, etc.) are ignored and exceptions are not thrown.
dlFetchOnError    = 0

; Enables logging of remote requests - their request and response headers, GET
; query, POST size, response status and length and so on. strftime() %X substrings
; can be used similar to logFile.
;
dlLog             = out/dl.log

; When dlLog exceeds this size it's cleared. Allows for K/M suffixes (Kilo/Megabyte).
; Note that it might be overwritten several times during single queue item processing
; if it does many requests.
;
dlLogMax          = 10M


;------------------------------------------------------------------------
; Class Autoloading
;------------------------------------------------------------------------

; Aliasing class to another class name.
class Sqobot\HLEx = Px\HLEx

; HLEx is a utility class required for web interface.
class Px\HLEx     = $hlex.php

; Sample autoloader mapping, overrides default search paths. Initial '$' refers
; to lib/. If path doesn't end on '.php' it's class -> class name alias (as above).
;class SomeClass   = $some.class.php

; Site name -> Sqissor mappings can be listed here. Class name is global (not relative
; to Sqobot\) and will be autoloaded using regular rules including 'class X=file.php'
; mappings above.
;
; If a site isn't listed here it's converted to 'Sqobot\S' + ucfirst(class name with
; removed dots and a symbol following them converted to upper case).
; Example: site.users -> Sqobot\SSiteUsers.
;
;class site.users  = MyNS\UsersParser


;------------------------------------------------------------------------
; URL Caching
;------------------------------------------------------------------------
;
; These settings allow you to store remote pages locally thus avoiding constant
; requests on the same address(es). Very useful for debugging.
;

; Sample URL mappings. They are evaluated recursively until a non-listen URL is
; found - it's then used to retrieve the data. Useful on local systems to speed
; up sqissor debugging saving remote pages locally. URLs are looked up exactly
; as requested (e.g. in download()).
;
;url http://site.name/list.php = file:///home/roger/sqissor/list-1.html
;url http://site.name/list.php?page=1 = http://site.name/list.php?page=0

; Relative paths start from Sqobot's working directory (usually of cli.php).
;url http://site.name/list.php = user/list-1.html

; Empty value terminates lookup and uses left-side URL as final.
;url http://site.name/list.php =

; Note that even local paths should be URL-encoded as they're converted to file:///.
;url http://site.name/list.php = path/with%20spaces.html

; If URL contains '=' you can escape it by doubling (and escape double '='s by
; tripling and so forth). This syntax is valid in any config option, not just 'url'.
;url http://site.name/req.php?a==b&abc=1 = out/cached-req.html

; This URL is translated from http://site.name/req.php?tripled==query.
;url http://site.name/req.php?tripled===query = out/cached-req.html



;------------------------------------------------------------------------
; Web Interface Access
;------------------------------------------------------------------------

; List of default permissions for anyone accessing this Sqobot instance via web
; interface. Perms starting with 'web' allow for web tasks with this name.
; Command-line user is always granted unrestricted access.
;
user                  = *
;webindex webstatus 

; Set to empty string to disallow web access unless allowed for an authorized
; user by more specific options below.
;user                  =

; If value starts with '=' default permissions ('user' with no name) are not used
; and this user always has exactly listed perms. If value is just '=' the user
; has no perms and is denied web access even if 'user' above allows it by default.
;
;user sqipper          = = webstatus webpages webcron cron-queue cron-url

; Linking PHP_AUTH_USER with his permissions in web interface.
; User name and permissions are case-sensitive.
;user HttpAuthLogin    = weblog

; Asterisk grants all permissions. Use for yourself.
;user aion            = *

cookiePrefix          = sqb-
; Either a number in seconds or a word (see Squall's expire()).
cookieExpire          = month
cookiePath            =
cookieDomain          =
; If empty string will be autoset if request is served via HTTPS or not.
cookieSecure          =

forceHTTPS            = 0

; By default user denials are rendered as 403 Forbidden. This option can be set to
; arbitrary HTTP code (e.g. 404) to conceal the fact that user lacks permissions.
;
; Since Sqobot's error response differs from your webserver's error page (Apache's,
; nginx', etc.) you might also want to set up ErrorDocument like .../web/?quit=404.
;
webDenyAs             = 403


;------------------------------------------------------------------------
; Web Interface Look
;------------------------------------------------------------------------

; Composition of the default web interface page. Asterisk includes all unlisted
; tasks in alphabetical order. If it's not present only listed tasks are shown.
; Tasks to which the user has no access are hidden. Duplicates are ignored.
;
webIndexOrder         = log tasks pages cron

; Links item captions in the top menu with their URL relative to web interface root.
webMenu Start         = .

webStyle common       = $common.css

webScript jquery      = //ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js
; Sample JS libraries loaded from a CDN.
;webScript underscore  = //cdnjs.cloudflare.com/ajax/libs/underscore.js/1.4.4/underscore-min.js
;webScript u.string    = //cdnjs.cloudflare.com/ajax/libs/underscore.string/2.3.0/underscore.string.min.js
webScript common      = $common.js


;------------------------------------------------------------------------
; Web Cron Jobs
;------------------------------------------------------------------------

; Assigning Sqobot task command lines to names accessible via a web request by
; invoking webcron task. Setting value is treated as if it was called from shell
; like ./cli - this includes 'values with spaces', --options and such.
;
; Useful on ; servers with unavailable crontab.
; Wrap arguments with spaces in " or ' (escaping like \" or \' isn't supported).
;
; Permissions are checked for webcron task itself and then for particular task
; as 'cron-TASK'. Perms for what is invoked on the right-side are not checked
; just like normal command-line calls.
;

; If set `webcron poll` will e-mail error reports to this address.
;webcronMailErrors  = me@example

; If set `webcron poll` will e-mail successfully ran tasks' outputs to this address.
;webcronMailOK      = me@example

;webcron queue     = queue --mark --delay=100
;webcron qurl      = queue url

;webcron sample    = task 'with spaces' --option="" "--one-quote='"


;------------------------------------------------------------------------
; scan constants
;------------------------------------------------------------------------
;    sitename      site.index  site.page   table_index table_pages start_url
scan kickstarter = kickstarter.index kickstarter.page kickstarter_index kickstarter_pages http://www.kickstarter.com/discover/recently-launched?page=1
scan indiegogo   = indiegogo.index indiegogo.page indiegogo_index indiegogo_pages http://www.indiegogo.com/projects?filter_quick=new&pg_num=1

